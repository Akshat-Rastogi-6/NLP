{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MB511WS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client busy unable discuss also still meeting clients onboarding awaiting feedback\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # remove special character\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "sample_text = \"Client been very busy - unable to discuss Also still meeting with clients about onboarding them Awaiting further feedback\"\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word token ['client', 'busy', 'unable', 'discuss', 'also', 'still', 'meeting', 'clients', 'onboarding', 'awaiting', 'feedback']\n",
      "sentence token ['client busy unable discuss also still meeting clients onboarding awaiting feedback']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize_word(text) :\n",
    "    word_token = word_tokenize(text)\n",
    "    sent_token = sent_tokenize(text)\n",
    "\n",
    "    return word_token, sent_token\n",
    "\n",
    "word_token, sent_token = tokenize_word(cleaned_text)\n",
    "print(\"word token\", word_token)\n",
    "print(\"sentence token\", sent_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Text: client busy unable discus also still meeting client onboarding awaiting feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MB511WS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MB511WS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmentizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmantize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmantize_token = [lemmentizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmantize_token)\n",
    "\n",
    "lemmatized_text = lemmantize_text(cleaned_text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words representation:\n",
      " [[1 1 1 2 1 1 1 1 1 1]]\n",
      "TF-IDF representation:\n",
      " [[0.2773501 0.2773501 0.2773501 0.5547002 0.2773501 0.2773501 0.2773501\n",
      "  0.2773501 0.2773501 0.2773501]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform([lemmatized_text])\n",
    "print(\"Bag of Words representation:\\n\", X_bow.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform([lemmatized_text])\n",
    "print(\"TF-IDF representation:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram representation:\n",
      " [[1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))\n",
    "X_ngram = vectorizer_ngram.fit_transform([lemmatized_text])\n",
    "print(\"N-gram representation:\\n\", X_ngram.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags [('client', 'NOUN'), ('busy', 'ADJ'), ('unable', 'ADJ'), ('discus', 'NOUN'), ('also', 'ADV'), ('still', 'ADV'), ('meeting', 'VERB'), ('client', 'NOUN'), ('onboarding', 'NOUN'), ('awaiting', 'VERB'), ('feedback', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "pos_tags =  pos_tagging(lemmatized_text)\n",
    "print(\"POS Tags\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities []\n"
     ]
    }
   ],
   "source": [
    "def name_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "entities = name_entity_recognition(lemmatized_text)\n",
    "print(\"Named Entities\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HANDLING IMBALANCE AND NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augemented Text:  ['client busy ineffectual discus besides still meet node onboarding awaiting feedback']\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "def augment_text(text):\n",
    "    aug = naw.SynonymAug(aug_src=\"wordnet\")\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "augmented_text = augment_text(lemmatized_text)\n",
    "print(\"Augemented Text: \", augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEEP LEARNING FOR NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.5187152028083801}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "# load pretrained model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# text classification\n",
    "nlp_classify = pipeline('text-classification', model = model, tokenizer = tokenizer)\n",
    "result = nlp_classify(augmented_text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
