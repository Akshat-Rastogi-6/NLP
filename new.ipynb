{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MB511WS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client busy unable discuss also still meeting clients onboarding awaiting feedback\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "    # remove special character\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "sample_text = \"Client been very busy - unable to discuss Also still meeting with clients about onboarding them Awaiting further feedback\"\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word token ['client', 'busy', 'unable', 'discuss', 'also', 'still', 'meeting', 'clients', 'onboarding', 'awaiting', 'feedback']\n",
      "sentence token ['client busy unable discuss also still meeting clients onboarding awaiting feedback']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize_word(text) :\n",
    "    word_token = word_tokenize(text)\n",
    "    sent_token = sent_tokenize(text)\n",
    "\n",
    "    return word_token, sent_token\n",
    "\n",
    "word_token, sent_token = tokenize_word(cleaned_text)\n",
    "print(\"word token\", word_token)\n",
    "print(\"sentence token\", sent_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Text: client busy unable discus also still meeting client onboarding awaiting feedback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MB511WS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MB511WS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmentizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmantize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmantize_token = [lemmentizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmantize_token)\n",
    "\n",
    "lemmatized_text = lemmantize_text(cleaned_text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT REPRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words representation:\n",
      " [[1 1 1 2 1 1 1 1 1 1]]\n",
      "TF-IDF representation:\n",
      " [[0.2773501 0.2773501 0.2773501 0.5547002 0.2773501 0.2773501 0.2773501\n",
      "  0.2773501 0.2773501 0.2773501]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform([lemmatized_text])\n",
    "print(\"Bag of Words representation:\\n\", X_bow.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform([lemmatized_text])\n",
    "print(\"TF-IDF representation:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram representation:\n",
      " [[1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ngram = CountVectorizer(ngram_range=(1, 2))\n",
    "X_ngram = vectorizer_ngram.fit_transform([lemmatized_text])\n",
    "print(\"N-gram representation:\\n\", X_ngram.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags [('client', 'NOUN'), ('busy', 'ADJ'), ('unable', 'ADJ'), ('discus', 'NOUN'), ('also', 'ADV'), ('still', 'ADV'), ('meeting', 'VERB'), ('client', 'NOUN'), ('onboarding', 'NOUN'), ('awaiting', 'VERB'), ('feedback', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "pos_tags =  pos_tagging(lemmatized_text)\n",
    "print(\"POS Tags\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities []\n"
     ]
    }
   ],
   "source": [
    "def name_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "entities = name_entity_recognition(lemmatized_text)\n",
    "print(\"Named Entities\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HANDLING IMBALANCE AND NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augemented Text:  ['client busy ineffectual discus besides still meet node onboarding awaiting feedback']\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "def augment_text(text):\n",
    "    aug = naw.SynonymAug(aug_src=\"wordnet\")\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text\n",
    "\n",
    "augmented_text = augment_text(lemmatized_text)\n",
    "print(\"Augemented Text: \", augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEEP LEARNING FOR NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.5187152028083801}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "# load pretrained model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# text classification\n",
    "nlp_classify = pipeline('text-classification', model = model, tokenizer = tokenizer)\n",
    "result = nlp_classify(augmented_text)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.171*\"this\" + 0.120*\"product.\" + 0.104*\"i\" + 0.089*\"is\" + 0.078*\"a\" + 0.069*\"fantastic\" + 0.068*\"product!\" + 0.066*\"do\" + 0.065*\"love\" + 0.063*\"not\"\n",
      "Topic: 1 \n",
      "Words: 0.134*\"product.\" + 0.130*\"this\" + 0.121*\"a\" + 0.102*\"is\" + 0.093*\"bad\" + 0.076*\"i\" + 0.064*\"like\" + 0.062*\"not\" + 0.058*\"love\" + 0.057*\"do\"\n",
      "[(0, 0.876012), (1, 0.12398801)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Example corpus\n",
    "documents = [\"This is a fantastic product!\", \"I love this product.\", \"I do not like this product.\", \"This is a bad product.\"]\n",
    "\n",
    "# Tokenize and create dictionary\n",
    "texts = [[word for word in document.lower().split()] for document in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create the bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train LDA model\n",
    "lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "\n",
    "# Print topics\n",
    "for idx, topic in lda.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n",
    "# Categorize new document\n",
    "new_doc = \"I love this fantastic product\"\n",
    "new_bow = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(lda.get_document_topics(new_bow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d4dc664be2428d99d1a06f20196fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MB511WS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MB511WS\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.3.0+cu121 with CUDA 1201 (you have 2.3.0+cpu)\n",
      "    Python  3.10.11 (you have 3.10.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: LABEL_0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the text classification pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "\n",
    "# Define your text for classification\n",
    "text = \"currently 5 clients using qbo\"\n",
    "\n",
    "# Perform text classification\n",
    "result = classifier(text)\n",
    "\n",
    "# Print the result\n",
    "print(\"Predicted label:\", result[0]['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
